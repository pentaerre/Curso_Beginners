{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "introduction_h5py.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz34n50pC5LS"
      },
      "source": [
        "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
        "-- | -- | --\n",
        "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
        " \n",
        "---\n",
        "\n",
        "<CENTER>\n",
        "<H1 style=\"color:red\">An Introduction to h5py </H1>\n",
        "</CENTER>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwkeaXlmC5LW"
      },
      "source": [
        "from __future__ import print_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO862WpJC5LX"
      },
      "source": [
        "# <font color='red'> Useful References </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xokcA_luC5LX"
      },
      "source": [
        "* <A HREF=\"https://buildmedia.readthedocs.org/media/pdf/h5py/latest/h5py.pdf\">h5py Documentation</A>\n",
        "* <A HREF=\"https://www.christopherlovell.co.uk/blog/2016/04/27/h5py-intro.html\">h5py: reading and writing HDF5 files in Python</A>\n",
        "* <A HREF=\"https://www.pythonforthelab.com/blog/how-to-use-hdf5-files-in-python/\">How to use HDF5 files in Python</A>\n",
        "* <a href=\"https://confluence.slac.stanford.edu/pages/viewpage.action?pageId=99485805\">How to access HDF5 data from Python</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIglgLV4C5LY"
      },
      "source": [
        "## <font color=\"red\"> What we will cover </font>\n",
        "* Opening a file\n",
        "* Dimension\n",
        "* Variables\n",
        "* Attributes\n",
        "* Writing data\n",
        "* Creating groups\n",
        "* Reading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Foh-z_N5C5LY"
      },
      "source": [
        "## <font color='red'> What is HDF5?</font>\n",
        "* HDF5 is a file format and library for storing scientific data. \n",
        "* It was designed to meet growing and ever-changing scientific data-storage and data-handling needs, to take advantage of the power and features of today's computing systems. \n",
        "* It supports files larger than 2 GB and  parallel I/O. \n",
        "* An HDF5 file is a container for two kinds of objects: \n",
        "   1. **Datasets**:, Array-like collections of data.\n",
        "   2. **Groups**: Folder-like containers that hold datasets and other groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBmizmfoC5LZ"
      },
      "source": [
        "## <font color='red'> What is h5py?</font>\n",
        "\n",
        "* h5py is the Python interface to the HDF5.\n",
        "* Provide easy-to-use high level interface, which allows you to store huge amounts of numerical data.\n",
        "* Easily manipulate that data from NumPy. \n",
        "* Use straightforward NumPy and Python metaphors, like dictionary and NumPy array syntax. \n",
        "* Within h5py, HDF5 groups work like dictionaries, and datasets work like NumPy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Ia_2aCC5LZ"
      },
      "source": [
        "import os\n",
        "import datetime as dt\n",
        "import six\n",
        "import numpy as np\n",
        "import h5py"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhvnuGetC5La"
      },
      "source": [
        "#### <font color='red'> Opening a netCDF File</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkpnC7osC5La"
      },
      "source": [
        "hdFileName = 'sample_hdf5.h5'\n",
        "modeType   = 'w'\n",
        "hdfid = h5py.File(hdFileName, modeType)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R_3Kft6C5La"
      },
      "source": [
        "* `modeType`: 'w', 'r+', 'r', or 'a'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDYP4U63C5Lb"
      },
      "source": [
        "print(hdfid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQxHHICyC5Lb"
      },
      "source": [
        "#### <font color='red'> Data Compression </font>\n",
        "* When saving data, you may opt for compressing it using different algorithms. \n",
        "* h5py supports a few compression filters such as GZIP, LZF, and SZIP. \n",
        "* When using one of the compression filters, the data will be processed on its way to the disk and it will be decompressed when reading it. \n",
        "* When readding data, the underlying HDF5 library automatically extracts the data from the compressed datasets with the appropriate algorithm.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlCX2q4cC5Lb"
      },
      "source": [
        "# gzip compression flag\n",
        "comp = 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n__P5hOjC5Lc"
      },
      "source": [
        "* Specify the data type to optimize space\n",
        "* Set data compression if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gGVsLDEC5Lc"
      },
      "source": [
        "#### <font color='red'> Creating Dimensions in a HDF5 File</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdrxRxQmC5Lc"
      },
      "source": [
        "# Latitude    \n",
        "lat = np.arange(-90,91,2.0)\n",
        "dset = hdfid.require_dataset('lat', shape=lat.shape, dtype=np.float32, \n",
        "                             compression=\"gzip\", compression_opts=comp)\n",
        "dset[...] = lat\n",
        "dset.attrs['name'] = 'latitude'\n",
        "dset.attrs['units'] = 'degrees north'\n",
        "\n",
        "# Longitude\n",
        "lon = np.arange(-180, 180, 2.5)\n",
        "dset = hdfid.require_dataset('lon', shape=lon.shape, dtype=np.float32, \n",
        "                             compression=\"gzip\", compression_opts=comp)\n",
        "dset[...] = lon\n",
        "dset.attrs['name'] = 'longitude'\n",
        "dset.attrs['units'] = 'degrees east'\n",
        "\n",
        "# Vertical levels\n",
        "lev = np.arange(0, 72, 1)\n",
        "dset = hdfid.require_dataset('lev', shape=lev.shape, dtype=np.int, \n",
        "                             compression=\"gzip\", compression_opts=comp)\n",
        "dset[...] = lev\n",
        "dset.attrs.update({'name': 'vertical levels',\n",
        "                   'units': 'hPa'})\n",
        "\n",
        "# Time (note the unlimited dimension)\n",
        "time = np.arange(0,1,1)\n",
        "dset = hdfid.require_dataset('time', shape=time.shape, maxshape=(None), \n",
        "                             dtype=np.float32, compression=comp)\n",
        "dset[...] = time\n",
        "dset.attrs['name'] = 'time'\n",
        "dset.attrs['units'] = 'hours since 2013-01-01 00:00:00.0'\n",
        "dset.attrs['calendar'] = 'gregorian'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F04VQjBHC5Ld"
      },
      "source": [
        "#### <font color='red'>Create Variables</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppRX_y1fC5Ld"
      },
      "source": [
        "nrecs = 5\n",
        "arr = np.zeros((nrecs,lev.size,lat.size,lon.size))\n",
        "arr[0:nrecs,:,:,:] = 300*np.random.uniform(size=(nrecs,lev.size,lat.size,lon.size))\n",
        "dset = hdfid.require_dataset('temp', shape=arr.shape, \n",
        "                             dtype=np.float32, compression=comp)\n",
        "dset[...] = arr\n",
        "dset.attrs['name'] = 'temperature'\n",
        "dset.attrs['units'] = 'K'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYHoKc0iC5Le"
      },
      "source": [
        "We can also use the `create_dataset` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhINMWHrC5Le"
      },
      "source": [
        "arr2 = np.zeros((lat.size,lon.size))\n",
        "arr2[:,:] = np.random.random(size=(lat.size,lon.size))\n",
        "landfrac = hdfid.create_dataset('landfrac', data=arr2, dtype=np.float32)\n",
        "landfrac.attrs['name'] = 'Fraction of land'\n",
        "landfrac.attrs['units'] = '1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW_TnQF1C5Lf"
      },
      "source": [
        "#### <font color='red'>Adding Global Attributes</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbFJajkbC5Lf"
      },
      "source": [
        "hdfid.attrs['Description'] = 'Sample HDF5 file'\n",
        "hdfid.attrs['History']     = 'Created for the h5py Tutorial'\n",
        "hdfid.attrs['Source']      = 'NASA GSFC'\n",
        "hdfid.attrs['HDF5_Version'] = six.u(h5py.version.hdf5_version)\n",
        "hdfid.attrs['h5py_version'] = six.u(h5py.version.version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpIv6RYOC5Lf"
      },
      "source": [
        "glob_attr = {'Date': dt.datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"), \n",
        "            'User': 'ASTG',}\n",
        "hdfid.attrs.update(glob_attr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWOzu-FqC5Lf"
      },
      "source": [
        "You can also use the formulation based on `json` to write metadata in the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4ldS_IzC5Lg"
      },
      "source": [
        "import json\n",
        "\n",
        "metadata = {'Note': 'Another way to add metadata in file', \n",
        "            'OS': os.name,}\n",
        "m = hdfid.create_dataset('metadata', data=json.dumps(metadata))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAwA6CV9C5Lg"
      },
      "source": [
        "#### <font color='red'>Printing File Attributes</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSG8iSgQC5Lh"
      },
      "source": [
        "for k in hdfid.attrs.keys():\n",
        "    print('{} => {}'.format(k, hdfid.attrs[k]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4r5k-9rC5Lh"
      },
      "source": [
        "metadata_read = json.loads(hdfid['metadata'][()])\n",
        "for k in metadata_read:\n",
        "    print('{} => {}'.format(k, metadata_read[k]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMA3pBABC5Lh"
      },
      "source": [
        "#### <font color='red'>List all Variables</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfZpslquC5Li"
      },
      "source": [
        "for item in hdfid:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PbaGqSJC5Li"
      },
      "source": [
        "#### <font color='red'>Create Groups</font>\n",
        "* We can organize data in hierarchical groups, which are analogous to directories in a filesystem. \n",
        "* Groups serve as containers for variables, dimensions and attributes, as well as other groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxKt2nAEC5Li"
      },
      "source": [
        "gpData2D = hdfid.create_group('2D_Data')\n",
        "gpData2D.attrs['Description'] = \"Group for 2D variables\"\n",
        "gpData2D.attrs['Sub groups'] = \"Land and Sea\"\n",
        "\n",
        "sgpLand  = gpData2D.create_group('2D_Land')\n",
        "sgpSea   = gpData2D.create_group('2D_Sea')\n",
        "\n",
        "gpData3D = hdfid.create_group('3D_Data')\n",
        "gpData3D.attrs['Description'] = \"Group for 2D variables\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhzRl818C5Li"
      },
      "source": [
        "# List all variables and top groups\n",
        "for item in hdfid:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgbyEdxPC5Lj"
      },
      "source": [
        "print('Parent of \"2D_Land\" is {}'.format(sgpLand.parent))\n",
        "print('Parent of \"3D_Data\" is {}'.format(gpData3D.parent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6DNNTN8C5Lj"
      },
      "source": [
        "for j in gpData2D.attrs.keys():\n",
        "    print('{} => {}'.format(j, gpData2D.attrs[j]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "divbXiCsC5Lj"
      },
      "source": [
        "for item in gpData2D:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeBSrX7QC5Lj"
      },
      "source": [
        "# Print all top variables and top groups in a file\n",
        "for key in hdfid.keys():\n",
        "    print(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVJ22rB3C5Lk"
      },
      "source": [
        "The `visit()` method allows to list all the nested groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zARstvD0C5Lk"
      },
      "source": [
        "def get_all(name):\n",
        "    print(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6y-DbFIC5Lk"
      },
      "source": [
        "hdfid.visit(get_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUuw50QJC5Lk"
      },
      "source": [
        "The `visititems()` method allows you to view all items in the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69vD-ODZC5Ll"
      },
      "source": [
        "results=[]\n",
        "def get_all_objs(name, obj):\n",
        "    attr = list(obj.attrs.items())\n",
        "    print(name, obj, attr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuIxbQT-C5Ll"
      },
      "source": [
        "hdfid.visititems(get_all_objs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQii7IT1C5Ll"
      },
      "source": [
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbJpPfnVC5Ll"
      },
      "source": [
        "##### Add variable to a group"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uuZqgQuC5Ll"
      },
      "source": [
        "arr2 = np.zeros((nrecs,lat.size,lon.size))\n",
        "arr2[0:nrecs,:,:] = 300*np.random.uniform(\n",
        "                                size=(nrecs,lat.size,lon.size))\n",
        "surf_pres = sgpLand.create_dataset('sp', data=arr2)\n",
        "surf_pres.attrs['name'] = 'Surface Pressure'\n",
        "surf_pres.attrs['units'] = 'Pa'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9T-j8A3C5Lm"
      },
      "source": [
        "temp = gpData3D.create_dataset('temp', data=arr)\n",
        "temp.attrs['name'] = 'temperature (3D)'\n",
        "temp.attrs['units'] = 'K'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24Xazx6vC5Lm"
      },
      "source": [
        "print(hdfid[\"3D_Data\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW6IkOk-C5Lm"
      },
      "source": [
        "print(hdfid[\"2D_Data/2D_Land\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFPbAoPvC5Lm"
      },
      "source": [
        "for j in hdfid.attrs.keys():\n",
        "    print('{} => {}'.format(j, hdfid.attrs[j]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUAM6UbLC5Ln"
      },
      "source": [
        "for j in gpData2D.attrs.keys():\n",
        "    print('{} => {}'.format(j, gpData2D.attrs[j]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXvTrx4RC5Ln"
      },
      "source": [
        "#### <font color='red'>Close the file</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ_cdPOiC5Ln"
      },
      "source": [
        "hdfid.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoFSMXNhC5Ln"
      },
      "source": [
        "### <font color='red'>Reading a HDF5 File</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyiFhq5EC5Ln"
      },
      "source": [
        "with h5py.File('sample_hdf5.h5', 'r') as hdfid:\n",
        "     print(hdfid.keys())\n",
        "\n",
        "     lev  = hdfid['lev'][()]\n",
        "     lat  = hdfid['lat'][()]\n",
        "     lon  = hdfid['lon'][()]\n",
        "     time = hdfid['time'][()]\n",
        "\n",
        "     temp1 = hdfid['temp'][()]\n",
        "     print(temp1[0,0,0,0], temp1[4,6,7,15])\n",
        "\n",
        "     temp2 = hdfid['3D_Data']['temp'][()]\n",
        "     print(temp2[0,0,0,0], temp2[4,6,7,15])     \n",
        "        \n",
        "     surf_pres = hdfid['2D_Data/2D_Land']['sp'][()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiAsQZN9C5Lo"
      },
      "source": [
        "print(lon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW18otDIC5Lo"
      },
      "source": [
        "print(lat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Rf98vxC5Lo"
      },
      "source": [
        "print(temp1.shape)\n",
        "print(np.min(temp1), np.max(temp1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6TZYJcGC5Lo"
      },
      "source": [
        "print(temp2.shape)\n",
        "print(np.min(temp2), np.max(temp2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suvznZW5C5Lo"
      },
      "source": [
        "print(surf_pres.shape)\n",
        "print(np.min(surf_pres), np.max(surf_pres))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBVvWkI3C5Lp"
      },
      "source": [
        "**List the names of datasets:** Use `visit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMdjucd7C5Lp"
      },
      "source": [
        "with h5py.File(hdFileName, 'r') as hf:\n",
        "     hf.visit(print)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ey9jKAVC5Lp"
      },
      "source": [
        "**List the names of the datasets and the corresponding objects**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLLf8DnWC5Lp"
      },
      "source": [
        "def my_func(name):\n",
        "    print(name, hf[name])\n",
        "\n",
        "with h5py.File(hdFileName, 'r') as hf:\n",
        "     hf.visit(my_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2jMS--vC5Lq"
      },
      "source": [
        "**List the datasets and their attributes:** Use `visititems`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aKPoeTAC5Lq"
      },
      "source": [
        "def printall(name, obj):\n",
        "    print(name, dict(obj.attrs))\n",
        "\n",
        "with h5py.File(hdFileName, 'r') as hf:\n",
        "     hf.visititems(printall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oSyxbaNC5Lq"
      },
      "source": [
        "def printall(name, obj):\n",
        "    if isinstance(obj, h5py.Group):\n",
        "        print(name, \" is a Group\")\n",
        "    elif isinstance(obj, h5py.Dataset):\n",
        "        print(name, \" is a Dataset\")\n",
        "    else:\n",
        "        print(name, \" is of an unknown type\")\n",
        "\n",
        "with h5py.File(hdFileName, 'r') as hf:\n",
        "     hf.visititems(printall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-VCBlKEC5Lq"
      },
      "source": [
        "### Get information about HDF5 item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NooDT9UqC5Lq"
      },
      "source": [
        "**For all HDF5 items:**\n",
        "\n",
        "```python\n",
        "item.id     \n",
        "item.ref     \n",
        "item.parent  \n",
        "item.file   \n",
        "item.name    \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHq4pa00C5Lr"
      },
      "source": [
        "with h5py.File('sample_hdf5.h5', 'r') as hdfid:\n",
        "     mylist = list(hdfid.keys())\n",
        "     for var in mylist:\n",
        "         obj = hdfid[var]\n",
        "         #if isinstance(obj, h5py.Dataset):\n",
        "         print(obj.name, obj.parent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPjI6rSZC5Lr"
      },
      "source": [
        "**For Dataset**\n",
        "\n",
        "```python\n",
        "ds.dtype     \n",
        "ds.shape     \n",
        "ds.value     \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twIef76NC5Lr"
      },
      "source": [
        "with h5py.File('sample_hdf5.h5', 'r') as hdfid:\n",
        "     mylist = list(hdfid.keys())\n",
        "     for var in mylist:\n",
        "         obj = hdfid[var]\n",
        "         if isinstance(obj, h5py.Dataset):\n",
        "             print(var, obj.dtype, obj.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2krXd6QoC5Lr"
      },
      "source": [
        "**Get item attributes for File or Group (if attributes available)**\n",
        "\n",
        "```python\n",
        "item.attrs          # for example: <Attributes of HDF5 object at 230141696>\n",
        "item.attrs.keys()   # for example: ['start.seconds', 'start.nanoseconds']\n",
        "item.attrs.values() # for example: [1297608424L, 627075857L]\n",
        "len(item.attrs)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq9nDbuCC5Lr"
      },
      "source": [
        "with h5py.File('sample_hdf5.h5', 'r') as hdfid:\n",
        "     mylist = list(hdfid.keys())\n",
        "     for var in mylist:\n",
        "         obj = hdfid[var]\n",
        "         print(obj.attrs.keys(), len(obj.attrs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79bTYAA8C5Ls"
      },
      "source": [
        "with h5py.File('sample_hdf5.h5', 'r') as hdfid:\n",
        "     for param_key, param in hdfid.items():\n",
        "         msg = '{}:'.format(param_key)\n",
        "         if isinstance(param, h5py.Dataset):\n",
        "             msg += ' {}'.format(param.shape)\n",
        "         print(msg)\n",
        "         if isinstance(param, h5py.Group):\n",
        "             for child_key, child in param.items():\n",
        "                 if isinstance(child, h5py.Dataset):\n",
        "                     print('     {}: {}'.format(child_key, child.shape))\n",
        "                 else:\n",
        "                     print('     {}: {}'.format(child_key, child.parent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPDzAYUtC5Ls"
      },
      "source": [
        "**List everything in the file:** `h5ls`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ebzsF78C5Ls"
      },
      "source": [
        "def print_attrs(name, obj):\n",
        "    shift = name.count('/') * '    '\n",
        "    print(shift + name)\n",
        "    for key, val in obj.attrs.items():\n",
        "        print(shift + '    ' + f\"{key}: {val}\")\n",
        "        \n",
        "with h5py.File('sample_hdf5.h5', mode='r') as hdfid:\n",
        "     hdfid.visititems(print_attrs)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9PZS1SXC5Ls"
      },
      "source": [
        "**Get the list of all the datasets and their values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz0tTBmdC5Lt"
      },
      "source": [
        "dct = dict()\n",
        "def get_data(name, obj, dct=dct):\n",
        "    if isinstance(obj, h5py.Dataset):\n",
        "        _name = name if name.startswith('/') else '/'+name\n",
        "        dct[_name] = obj[()]\n",
        "\n",
        "with h5py.File('sample_hdf5.h5', mode='r') as hdfid:         \n",
        "     hdfid.visititems(get_data)\n",
        "    \n",
        "\n",
        "for key in dct:\n",
        "    print(\"{}:\".format(key, dct[key]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6cKjcZqC5Lt"
      },
      "source": [
        "### <font color='red'>Updating a Variable in an Existing HDF5 File</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WduIfd9FC5Lt"
      },
      "source": [
        "with h5py.File(hdFileName, mode='a') as hdfid:\n",
        "     tempO = hdfid['temp']  # Read the object associated with the dataset temp\n",
        "     data = tempO[:]        # Extract the values\n",
        "     print(np.min(data), np.max(data))\n",
        "     data = 1.1*data + 100.0\n",
        "     tempO[:] = data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VCwv__hC5Lt"
      },
      "source": [
        "print(data.shape)\n",
        "print(np.min(data), np.max(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qXMx0E3C5Lt"
      },
      "source": [
        "with h5py.File(hdFileName, mode='r') as hdfid:\n",
        "     tempV = hdfid['temp'][()]\n",
        "     print(np.min(tempV), np.max(tempV))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwDV1endC5Lu"
      },
      "source": [
        "### <font color='red'>Resizing Datasets</font>\n",
        "* HDF5 allows resizing datasets on the fly and with little computational cost. \n",
        "* Datasets can be resized once created up to a maximum size. \n",
        "* You need to specify this maximum size when creating the dataset, via the keyword `maxshape`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4zbFn3eC5Lu"
      },
      "source": [
        "**Example:**\n",
        "\n",
        "- Create a dataset to store `100` values and set a maximum size of up to `500` (`maxshape=(500, )`) values.\n",
        "- Populate the first `100` entries of the dataset.\n",
        "- Expand (new size of `200`) the dataset to store `100` more entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH8kQf-GC5Lu"
      },
      "source": [
        "with h5py.File('resize_dataset.h5', 'w') as hdfid:\n",
        "    d = hdfid.create_dataset('dataset', (100, ),  maxshape=(500, ))\n",
        "    d[:100] = np.random.uniform(0.0,99.0, (100,))\n",
        "    d.resize((200,))\n",
        "    d[100:200] = np.random.uniform(100,199.0, (100,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEZwnPHfC5Lv"
      },
      "source": [
        "You can open the file and read the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCJrUubhC5Lv"
      },
      "source": [
        "with h5py.File('resize_dataset.h5', 'r') as hdfid:\n",
        "    dset = hdfid['dataset']\n",
        "    print(dset[99])\n",
        "    print(dset[199])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ANOBlOGC5Lv"
      },
      "source": [
        "You can also resize the dataset at a later stage, don't need to do it in the same session when you created the file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RCaPxBnC5Lv"
      },
      "source": [
        "with h5py.File('resize_dataset.h5', 'a') as hdfid:\n",
        "    dset = hdfid['dataset']\n",
        "    dset.resize((300,))\n",
        "    #dset[:200] = 0\n",
        "    dset[200:300] = np.random.uniform(200, 299.0, (100,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP2RrbHJC5Lv"
      },
      "source": [
        "with h5py.File('resize_dataset.h5', 'r') as hdfid:\n",
        "    dset = hdfid['dataset']\n",
        "    print(dset[99])\n",
        "    print(dset[199])\n",
        "    print(dset[299])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icCHfqGfC5Lw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}